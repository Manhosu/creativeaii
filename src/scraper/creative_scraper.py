"""
Scraper espec√≠fico para Creative C√≥pias
Implementa m√©todos de extra√ß√£o para https://www.creativecopias.com.br
"""

from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
import requests
import re
import os
from loguru import logger
from .scraper_base import ScraperBase
from .pagination_handler import PaginationHandler

class CreativeScraper(ScraperBase):
    """Scraper espec√≠fico para Creative C√≥pias"""
    
    def __init__(self, delay_range: tuple = (2, 4), timeout: int = 30):
        """
        Inicializa scraper do Creative C√≥pias
        
        Args:
            delay_range: Delay entre requests (2-4 segundos)
            timeout: Timeout para requests
        """
        super().__init__(delay_range, timeout)
        self.base_url = os.getenv("SITE_BASE_URL", "https://www.creativecopias.com.br")
        
        # Headers espec√≠ficos para Creative C√≥pias (sem User-Agent espec√≠fico)
        # Criar nova sess√£o limpa para evitar headers herdados
        self.session.close()
        self.session = requests.Session()
            
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # Inicializar handler de pagina√ß√£o
        self.pagination_handler = PaginationHandler(delay_range=(1, 2))
        
        logger.info("üöÄ Creative Scraper inicializado")
    
    def load_page(self, url: str) -> Optional[BeautifulSoup]:
        """
        Carrega p√°gina da Creative C√≥pias
        
        Args:
            url: URL para carregar
            
        Returns:
            BeautifulSoup object ou None se erro
        """
        try:
            logger.info(f"üìÑ Carregando p√°gina: {url}")
            
            # Usar requests simples sem User-Agent espec√≠fico (funciona melhor com Creative C√≥pias)
            response = requests.get(url, timeout=self.timeout)
            
            # Verificar se p√°gina carregou corretamente
            if "creative" not in response.text.lower():
                logger.warning(f"‚ö†Ô∏è P√°gina pode n√£o ter carregado corretamente: {url}")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            logger.debug(f"‚úÖ P√°gina carregada com sucesso: {len(response.content)} bytes")
            
            return soup
            
        except requests.RequestException as e:
            logger.error(f"‚ùå Erro de rede ao carregar {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erro ao processar p√°gina {url}: {e}")
            return None
    
    def parse_product_list(self, html: BeautifulSoup) -> List[Any]:
        """
        Extrai elementos de produtos da p√°gina
        
        Args:
            html: BeautifulSoup da p√°gina
            
        Returns:
            Lista de elementos de produtos
        """
        try:
            products = []
            
            logger.info("üîç Iniciando parse_product_list...")
            
            # Primeiro, buscar especificamente por elementos com .product-name (detectado na an√°lise)
            product_names = html.select('.product-name')
            logger.info(f"üì¶ Busca por .product-name retornou {len(product_names)} elementos")
            
            if product_names:
                logger.info(f"üì¶ Encontrados {len(product_names)} elementos .product-name")
                for i, name_elem in enumerate(product_names):
                    logger.debug(f"  [{i+1}] Processando: {name_elem.get_text(strip=True)[:50]}...")
                    # Pegar o container pai do produto
                    parent = name_elem.find_parent(['li', 'div', 'article'])
                    if parent and parent not in products:
                        products.append(parent)
                        logger.debug(f"      ‚úÖ Produto {i+1} adicionado")
            
            if products:
                logger.info(f"üì¶ {len(products)} produtos encontrados via .product-name")
                logger.info(f"üéØ Total de {len(products)} produtos prontos para extra√ß√£o")
                return products
            
            # Se n√£o encontrou, tenta seletores espec√≠ficos
            selectors = [
                '.item',  # Priorizar .item que funciona para cartuchos/toners
                '.products-grid .item',  # Magento products grid items
                '.category-products .item',  # Magento category products
                '.item .product-name',  # Items com product name dentro
                '.product-item',  # Padr√£o Magento 2
                '.product',
                '.item-product',
                '[data-product]',
                '.card-product',
                '.product-card',
                '.showcase-item',
                'li[id*="product"]',
                '.vitrine-produto',
                '.produto-item'
            ]
            
            for selector in selectors:
                elements = html.select(selector)
                if elements:
                    logger.info(f"üì¶ Encontrados {len(elements)} produtos usando seletor: {selector}")
                    # Filtrar elementos que realmente parecem produtos
                    valid_products = []
                    for element in elements:
                        # Verificar se tem caracter√≠sticas de produto
                        has_name = element.select_one('.product-name, h2, h3, a[title]')
                        has_price = element.select_one('.price-box, .price, .preco')
                        has_link = element.select_one('a[href]')
                        
                        if has_name or has_price or has_link:
                            valid_products.append(element)
                    
                    if valid_products:
                        logger.info(f"‚úÖ {len(valid_products)} produtos v√°lidos encontrados com {selector}")
                        products.extend(valid_products)
                        break
            
            # Se n√£o encontrou com seletores espec√≠ficos, tenta busca mais gen√©rica
            if not products:
                # Buscar por links que contenham palavras relacionadas a produtos
                links = html.find_all('a', href=True)
                product_links = []
                
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True).lower()
                    
                    # Verificar se √© link de produto baseado na URL ou texto
                    if any(keyword in href.lower() for keyword in ['produto', 'item', '/p/', '/product']):
                        product_links.append(link.parent or link)
                    elif any(keyword in text for keyword in ['impressora', 'multifuncional', 'toner', 'cartucho']):
                        product_links.append(link.parent or link)
                
                products = list(set(product_links))  # Remove duplicatas
                
                if products:
                    logger.info(f"üì¶ Encontrados {len(products)} produtos via busca gen√©rica")
            
            # Se ainda n√£o encontrou, buscar por imagens de produtos
            if not products:
                img_elements = html.find_all('img', src=True)
                for img in img_elements:
                    src = img.get('src', '').lower()
                    alt = img.get('alt', '').lower()
                    
                    if any(keyword in src for keyword in ['produto', 'product', 'item']) or \
                       any(keyword in alt for keyword in ['impressora', 'multifuncional', 'toner']):
                        parent = img.find_parent(['div', 'li', 'article', 'section'])
                        if parent:
                            products.append(parent)
                
                products = list(set(products))  # Remove duplicatas
                
                if products:
                    logger.info(f"üì¶ Encontrados {len(products)} produtos via an√°lise de imagens")
            
            if not products:
                logger.warning("‚ö†Ô∏è Nenhum produto encontrado na p√°gina")
                # Debug: salvar HTML para an√°lise
                with open('logs/debug_page.html', 'w', encoding='utf-8') as f:
                    f.write(str(html))
                logger.info("üîç HTML da p√°gina salvo em logs/debug_page.html para an√°lise")
            else:
                logger.info(f"üéØ Total de {len(products)} produtos prontos para extra√ß√£o")
            
            return products
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair lista de produtos: {e}")
            return []
    
    def extract_product_data(self, product_element: Any) -> Dict[str, Any]:
        """
        Extrai dados de um produto espec√≠fico
        
        Args:
            product_element: Elemento HTML do produto
            
        Returns:
            Dicion√°rio com dados do produto
        """
        try:
            product_data = {}
            
            # Extrair nome do produto
            name = self._extract_product_name(product_element)
            if not name:
                return {}  # Nome √© obrigat√≥rio
            
            product_data['nome'] = name
            
            # Extrair URL do produto
            url = self._extract_product_url(product_element)
            product_data['url'] = url
            
            # Extrair pre√ßo
            price = self._extract_product_price(product_element)
            product_data['preco'] = price
            
            # Extrair c√≥digo do produto
            codigo = self._extract_product_code(product_element)
            product_data['codigo'] = codigo
            
            # Extrair marca
            marca = self._extract_product_brand(product_element)
            product_data['marca'] = marca
            
            # Extrair descri√ß√£o
            descricao = self._extract_product_description(product_element)
            product_data['descricao'] = descricao
            
            # Extrair imagem
            imagem = self._extract_product_image(product_element)
            product_data['imagem'] = imagem
            
            # Extrair disponibilidade
            disponivel = self._extract_product_availability(product_element)
            product_data['disponivel'] = disponivel
            
            logger.debug(f"‚úÖ Dados extra√≠dos para: {name}")
            return product_data
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair dados do produto: {e}")
            return {}
    
    def _extract_product_name(self, element: Any) -> Optional[str]:
        """Extrai nome do produto"""
        selectors = [
            '.product-name', '.nome-produto', '.title',  # Seletores espec√≠ficos primeiro
            'a[title]',  # Links com t√≠tulo
            'a',  # Qualquer link (para elementos .item)
            'h3', 'h2', 'h4',  # Headers
            '[data-name]', '.product-title'
        ]
        
        for selector in selectors:
            try:
                found = element.select_one(selector)
                if found:
                    # Primeiro tentar atributo title
                    if hasattr(found, 'get') and found.get('title'):
                        title = found.get('title').strip()
                        if len(title) > 3:
                            return title
                    
                    # Depois tentar texto do elemento
                    text = found.get_text(strip=True)
                    if len(text) > 3:  # Nome v√°lido deve ter mais de 3 caracteres
                        return text
            except:
                continue
        
        # √öltima tentativa: pegar primeiro texto significativo
        text = element.get_text(strip=True)
        if text and len(text) > 3:
            # Pegar primeira linha que pare√ßa um nome de produto
            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 10 and not line.startswith('R$') and not line.lower().startswith('marca'):
                    return line
        
        return None
    
    def _extract_product_url(self, element: Any) -> Optional[str]:
        """Extrai URL do produto"""
        try:
            # Buscar link dentro do elemento
            link = element.find('a', href=True)
            if link:
                href = link.get('href')
                
                # Se for URL relativa, adicionar dom√≠nio
                if href.startswith('/'):
                    return self.base_url + href
                elif href.startswith('http'):
                    return href
                else:
                    return self.base_url + '/' + href
        except:
            pass
        
        return None
    
    def _extract_product_price(self, element: Any) -> Optional[str]:
        """Extrai pre√ßo do produto"""
        price_selectors = [
            '.price', '.preco', '.valor',
            '[data-price]', '.product-price',
            '.money', '.currency'
        ]
        
        for selector in price_selectors:
            try:
                price_elem = element.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    # Extrair n√∫meros e v√≠rgulas/pontos do pre√ßo
                    price_match = re.search(r'R\$[\s]*([0-9.,]+)', price_text)
                    if price_match:
                        return f"R$ {price_match.group(1)}"
            except:
                continue
        
        # Busca mais gen√©rica por padr√£o de pre√ßo
        text = element.get_text()
        price_pattern = r'R\$[\s]*([0-9.,]+)'
        match = re.search(price_pattern, text)
        if match:
            return f"R$ {match.group(1)}"
        
        return None
    
    def _extract_product_code(self, element: Any) -> Optional[str]:
        """Extrai c√≥digo do produto"""
        try:
            text = element.get_text()
            
            # Padr√µes comuns de c√≥digo
            patterns = [
                r'C√≥digo[:\s]*([A-Z0-9\-]+)',
                r'Ref[:\s]*([A-Z0-9\-]+)',
                r'SKU[:\s]*([A-Z0-9\-]+)',
                r'Item[:\s]*([A-Z0-9\-]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    return match.group(1)
        except:
            pass
        
        return None
    
    def _extract_product_brand(self, element: Any) -> Optional[str]:
        """Extrai marca do produto"""
        try:
            brand_selectors = [
                '.brand', '.marca', '.fabricante',
                '[data-brand]', '.manufacturer'
            ]
            
            for selector in brand_selectors:
                brand_elem = element.select_one(selector)
                if brand_elem:
                    return brand_elem.get_text(strip=True)
            
            # Buscar marcas conhecidas no texto
            text = element.get_text().lower()
            brands = ['hp', 'canon', 'epson', 'brother', 'samsung', 'lexmark', 'xerox', 'ricoh']
            
            for brand in brands:
                if brand in text:
                    return brand.upper()
        except:
            pass
        
        return None
    
    def _extract_product_description(self, element: Any) -> Optional[str]:
        """Extrai descri√ß√£o do produto"""
        try:
            desc_selectors = [
                '.description', '.descricao', '.resumo',
                '.product-description', '.summary'
            ]
            
            for selector in desc_selectors:
                desc_elem = element.select_one(selector)
                if desc_elem:
                    desc = desc_elem.get_text(strip=True)
                    if len(desc) > 20:  # Descri√ß√£o v√°lida deve ser mais longa
                        return desc[:200]  # Limitar tamanho
        except:
            pass
        
        return None
    
    def _extract_product_image(self, element: Any) -> Optional[str]:
        """
        Extrai URL da imagem do produto com fallback robusto
        Garante sempre URLs ABSOLUTAS da Creative C√≥pias
        """
        try:
            # Seletores espec√≠ficos para Creative C√≥pias (em ordem de prioridade)
            img_selectors = [
                # Espec√≠ficos do Creative C√≥pias
                '.product-image img',         # Container principal de imagem
                '.item-image img',           # Imagem do item
                '.product-photo img',        # Foto do produto
                'img.product-image',         # Classe direta
                '.product-image-main img',   # Imagem principal
                '.product-thumb img',        # Thumbnail do produto
                
                # Seletores de atributos espec√≠ficos
                'img[alt*="produto" i]',     # Alt contendo "produto" (case insensitive)
                'img[alt*="item" i]',        # Alt contendo "item"
                'img[src*="/media/" i]',     # Imagens do diret√≥rio media
                'img[src*="/images/" i]',    # Imagens do diret√≥rio images
                'img[src*="/uploads/" i]',   # Imagens de uploads
                'img[src*="product" i]',     # Src contendo "product"
                
                # Fallback geral (com filtros)
                'img'
            ]
            
            for selector in img_selectors:
                try:
                    img = element.select_one(selector)
                    if not img:
                        continue
                    
                    # Tentar diferentes atributos de imagem (lazy loading)
                    src = (img.get('src') or 
                          img.get('data-src') or 
                          img.get('data-original') or 
                          img.get('data-lazy') or
                          img.get('data-image') or
                          img.get('data-img'))
                    
                    if not src or not src.strip():
                        continue
                    
                    src = src.strip()
                    
                    # FILTROS DE QUALIDADE - Rejeitar imagens inv√°lidas
                    if self._is_invalid_image(src):
                        logger.debug(f"üö´ Imagem rejeitada por filtro: {src}")
                        continue
                    
                    # CONVERTER PARA URL ABSOLUTA
                    absolute_url = self._make_absolute_url(src)
                    
                    if absolute_url:
                        logger.info(f"üñºÔ∏è Imagem extra√≠da: {absolute_url}")
                        return absolute_url
                        
                except Exception as selector_error:
                    logger.debug(f"‚ùå Erro no seletor {selector}: {selector_error}")
                    continue
            
            logger.warning(f"‚ö†Ô∏è Nenhuma imagem v√°lida encontrada no elemento")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao extrair imagem: {e}")
            return None
    
    def _is_invalid_image(self, src: str) -> bool:
        """
        Verifica se a URL da imagem deve ser rejeitada
        
        Args:
            src: URL da imagem a verificar
            
        Returns:
            True se a imagem √© inv√°lida
        """
        if not src:
            return True
        
        src_lower = src.lower()
        
        # Filtros de tamanho (imagens muito pequenas)
        size_filters = ['1x1', '10x10', '20x20', '0x0', 'px.gif', 'spacer', 'blank']
        if any(size in src_lower for size in size_filters):
            return True
        
        # Filtros de tipo (imagens de controle/sistema)
        type_filters = [
            'placeholder', 'loading', 'spinner', 'ajax-loader',
            'transparent', 'clear', 'empty', 'dummy', 'fake',
            'icon-', 'sprite', 'css', 'background', 'border',
            'arrow', 'button', 'nav', 'menu', 'close', 'search'
        ]
        if any(filter_type in src_lower for filter_type in type_filters):
            return True
        
        # Filtros de extens√£o inv√°lida
        invalid_extensions = ['.css', '.js', '.txt', '.html', '.php']
        if any(src_lower.endswith(ext) for ext in invalid_extensions):
            return True
        
        # Filtros de URL suspeitas
        suspicious_patterns = [
            'data:image/gif;base64,R0lGOD',  # GIF transparente comum
            'data:image/svg+xml',           # SVG inline muito simples
            'javascript:',                   # URLs JavaScript
            'mailto:',                      # URLs de email
            '#'                            # Links √¢ncora
        ]
        if any(pattern in src for pattern in suspicious_patterns):
            return True
        
        # Se passou por todos os filtros, √© v√°lida
        return False
    
    def _make_absolute_url(self, src: str) -> Optional[str]:
        """
        Converte URL relativa para absoluta usando base da Creative C√≥pias
        
        Args:
            src: URL da imagem (pode ser relativa ou absoluta)
            
        Returns:
            URL absoluta v√°lida ou None
        """
        if not src:
            return None
        
        # Se j√° √© absoluta e v√°lida, retornar
        if src.startswith('http'):
            # Verificar se √© do dom√≠nio Creative C√≥pias ou confi√°vel
            trusted_domains = [
                'creativecopias.com.br',
                'via.placeholder.com',
                'images.unsplash.com',
                'picsum.photos'
            ]
            
            if any(domain in src for domain in trusted_domains):
                return src
            else:
                # URL externa - logar mas retornar mesmo assim
                logger.debug(f"üìé URL externa aceita: {src}")
                return src
        
        # URLs data: s√£o v√°lidas mas n√£o ideais
        if src.startswith('data:'):
            logger.debug(f"üìä Data URL encontrada: {src[:50]}...")
            return src
        
        # Converter URL relativa para absoluta
        base_url = getattr(self, 'base_url', 'https://www.creativecopias.com.br')
        
        if src.startswith('/'):
            # URL relativa √† raiz
            absolute_url = f"{base_url}{src}"
        elif src.startswith('./'):
            # URL relativa ao diret√≥rio atual
            absolute_url = f"{base_url}/{src[2:]}"
        elif src.startswith('../'):
            # URL relativa ao diret√≥rio pai (n√£o comum em produtos)
            logger.warning(f"‚ö†Ô∏è URL relativa ao diret√≥rio pai: {src}")
            absolute_url = f"{base_url}/{src}"
        else:
            # URL relativa simples
            absolute_url = f"{base_url}/{src}"
        
        # Limpar URL duplicada (evitar ///)
        absolute_url = absolute_url.replace('///', '//')
        if absolute_url.count('//') > 1:
            absolute_url = absolute_url.replace('//', '/', 1).replace('//', '/')
            absolute_url = absolute_url.replace('http:/', 'http://').replace('https:/', 'https://')
        
        logger.debug(f"üîó URL convertida: {src} ‚Üí {absolute_url}")
        return absolute_url
    
    def _extract_product_availability(self, element: Any) -> bool:
        """Verifica se produto est√° dispon√≠vel"""
        try:
            text = element.get_text().lower()
            
            # Palavras que indicam indisponibilidade
            unavailable_words = ['indispon√≠vel', 'esgotado', 'fora de estoque', 'sem estoque']
            
            for word in unavailable_words:
                if word in text:
                    return False
            
            # Se encontrar bot√£o de comprar, provavelmente est√° dispon√≠vel
            buy_button = element.find(['button', 'a'], text=re.compile(r'comprar|adicionar', re.I))
            if buy_button:
                return True
                
        except:
            pass
        
        # Assumir dispon√≠vel por padr√£o
        return True
    
    def scrape_category_with_pagination(self, category_url: str, max_pages: int = 50) -> List[Dict[str, Any]]:
        """
        Scraping de categoria com pagina√ß√£o autom√°tica
        
        Args:
            category_url: URL da categoria
            max_pages: M√°ximo de p√°ginas para processar
            
        Returns:
            Lista de produtos de todas as p√°ginas
        """
        logger.info(f"üï∑Ô∏è Iniciando scraping com pagina√ß√£o: {category_url}")
        
        all_products = []
        
        try:
            # Usar pagination handler para obter todas as p√°ginas
            pages_data = self.pagination_handler.get_all_pages_from_category(
                category_url, 
                category_name=category_url.split('/')[-1]
            )
            
            for page_data in pages_data:
                page_num = page_data['page_number']
                logger.info(f"üìÑ Processando p√°gina {page_num}...")
                
                # Carregar a p√°gina para extrair dados completos
                soup = self.load_page(page_data['url'])
                if not soup:
                    logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar p√°gina {page_num}")
                    continue
                
                # Extrair produtos da p√°gina atual
                product_elements = self.parse_product_list(soup)
                
                if product_elements:
                    logger.info(f"üì¶ Encontrados {len(product_elements)} produtos na p√°gina {page_num}")
                    
                    # Extrair dados de cada produto
                    for i, element in enumerate(product_elements):
                        try:
                            product_data = self.extract_product_data(element)
                            if product_data and product_data.get('nome'):
                                # Adicionar informa√ß√µes da p√°gina
                                product_data['page_number'] = page_num
                                product_data['page_url'] = page_data['url']
                                all_products.append(product_data)
                                logger.debug(f"‚úÖ Produto {i+1} extra√≠do: {product_data['nome'][:50]}...")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Erro ao extrair produto {i+1}: {e}")
                            continue
                else:
                    logger.warning(f"‚ö†Ô∏è Nenhum produto encontrado na p√°gina {page_num}")
                
                # Delay entre p√°ginas
                self._apply_delay()
            
            logger.info(f"üéØ Scraping com pagina√ß√£o conclu√≠do: {len(all_products)} produtos "
                       f"encontrados em {len(pages_data)} p√°ginas")
            
            return all_products
            
        except Exception as e:
            logger.error(f"‚ùå Erro no scraping com pagina√ß√£o: {e}")
            return all_products
    
    def analyze_category_structure(self, category_url: str) -> Dict[str, Any]:
        """
        Analisa a estrutura de uma categoria (pagina√ß√£o, produtos, etc.)
        
        Args:
            category_url: URL da categoria para analisar
            
        Returns:
            An√°lise detalhada da categoria
        """
        logger.info(f"üîç Analisando estrutura da categoria: {category_url}")
        
        try:
            # Analisar pagina√ß√£o
            pagination_analysis = self.pagination_handler.analyze_pagination_structure(category_url)
            
            # Carregar primeira p√°gina para an√°lise de produtos
            soup = self.load_page(category_url)
            if not soup:
                return {'error': 'N√£o foi poss√≠vel carregar a p√°gina'}
            
            # Analisar produtos na primeira p√°gina
            product_elements = self.parse_product_list(soup)
            
            analysis = {
                'category_url': category_url,
                'pagination': pagination_analysis,
                'products_on_first_page': len(product_elements),
                'has_products': len(product_elements) > 0,
                'estimated_total_products': None
            }
            
            # Estimar total de produtos
            if pagination_analysis.get('has_pagination') and pagination_analysis.get('total_pages'):
                estimated_total = len(product_elements) * pagination_analysis['total_pages']
                analysis['estimated_total_products'] = estimated_total
                logger.info(f"üìä Estimativa: ~{estimated_total} produtos em {pagination_analysis['total_pages']} p√°ginas")
            else:
                analysis['estimated_total_products'] = len(product_elements)
                logger.info(f"üìä Categoria sem pagina√ß√£o: {len(product_elements)} produtos")
            
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao analisar categoria: {e}")
            return {'error': str(e)}
    
    def close(self):
        """Fecha conex√µes"""
        super().close()
        if hasattr(self, 'pagination_handler'):
            self.pagination_handler.close() 
 
 
 
 